Retrieve Superstep
The Retrieve superstep is a practical method for importing completely into the processing ecosystem a data lake
consisting of various external data sources. The Retrieve superstep is the first contact between your data science
and the source systems. I will guide you through a methodology of how to handle this discovery of the data up
to the point you have all the data you need to evaluate the system you are working with, by deploying your data
science skills. The successful retrieval of the data is a major stepping-stone to ensuring that you are performing
good data science. Data lineage delivers the audit trail of the data elements at the lowest granular level, to ensure
full data governance.
Data tagged in respective analytical models define the profile of the data that requires loading and guides the
data scientist to what additional processing is required.
A. Perform the following data processing using R.
Use R-Studio for the following:
> library(readr)
Warning message: package ‘readr’ was built under R version 3.4.4
Load a table named IP_DATA_ALL.csv.
> IP_DATA_ALL <- read_csv("C:/VKHCG/01-Vermeulen/00-RawData/IP_DATA_ALL.csv")
Parsed with column specification:
cols(
 ID = col_double(),
 Country = col_character(),
 `Place Name` = col_character(),
 `Post Code` = col_double(),
 Latitude = col_double(),
 Longitude = col_double(),
 `First IP Number` = col_double(),
 `Last IP Number` = col_double()
)
> View(IP_DATA_ALL)
> spec(IP_DATA_ALL)
cols(
 ID = col_double(),
 Country = col_character(),
 `Place Name` = col_character(),
 `Post Code` = col_double(),
 Latitude = col_double(),
 Longitude = col_double(),
 `First IP Number` = col_double(),
 `Last IP Number` = col_double()
)
This informs you that you have the following eight columns:
• ID of type integer
• Place name of type character
• Post code of type character
• Latitude of type numeric double
• Longitude of type numeric double
PSIT1P2 ~~~~~ Data Science Practical
M. Sc. [Information Technology ] SEMESTER ~ I Teacher’s Reference Manual
26
• First IP number of type integer
• Last IP number of type integer
> library(tibble)
> set_tidy_names(IP_DATA_ALL, syntactic = TRUE, quiet = FALSE)
New names:
Place Name -> Place.Name
Post Code -> Post.Code
First IP Number -> First.IP.Number
Last IP Number -> Last.IP.Number
This informs you that four of the field names are not valid and suggests new field names that are valid. You can
fix any detected invalid column names by executing
IP_DATA_ALL_FIX=set_tidy_names(IP_DATA_ALL, syntactic = TRUE, quiet = TRUE)
By using command View(IP_DATA_ALL_FIX), you can check that you have fixed the columns. The new table
IP_DATA_ALL_FIX.csv will fix the invalid column names with valid names.
> sapply(IP_DATA_ALL_FIX, typeof)
 ID Country Place.Name Post.Code Latitude
 "double" "character" "character" "double" "double"
 Longitude First.IP.Number Last.IP.Number
 "double" "double" "double"
> library(data.table)
> hist_country=data.table(Country=unique(IP_DATA_ALL_FIX[is.na(IP_DATA_ALL_FIX ['Country']) == 0, ]$Countr
y))
> setorder(hist_country,'Country')
> hist_country_with_id=rowid_to_column(hist_country, var = "RowIDCountry")
> View(hist_country_fix)
> IP_DATA_COUNTRY_FREQ=data.table(with(IP_DATA_ALL_FIX, table(Country)))
> View(IP_DATA_COUNTRY_FREQ)
• The two biggest subset volumes are from the US and GB.
• The US has just over four times the data as GB.
hist_latitude =data.table(Latitude=unique(IP_DATA_ALL_FIX [is.na(IP_DATA_ALL_with_ID ['Latitude']) == 0, ]$Lati
tude))
setkeyv(hist_latitude, 'Latitude')
setorder(hist_latitude)
hist_latitude_with_id=rowid_to_column(hist_latitude, var = "RowID")
View(hist_latitude_with_id)
IP_DATA_Latitude_FREQ=data.table(with(IP_DATA_ALL_FIX,table(Latitude)))
View(IP_DATA_Latitude_FREQ)
PSIT1P2 ~~~~~ Data Science Practical
M. Sc. [Information Technology ] SEMESTER ~ I Teacher’s Reference Manual
27
• The two biggest data volumes are from latitudes 51.5092 and 40.6888.
• The spread appears to be nearly equal between the top-two latitudes.
> sapply(IP_DATA_ALL_FIX[,'Latitude'], min, na.rm=TRUE)
Latitude 40.6888
What does this tell you?
Fact: The range of latitude for the Northern Hemisphere is from 0 to 90. So, if you do not have any latitudes
farther south than 40.6888, you can improve your retrieve routine.
> sapply(IP_DATA_ALL_FIX[,'Country'], min, na.rm=TRUE)
Country "DE"
Minimum business frequency is from DE – Denmark.
> sapply(IP_DATA_ALL_FIX[,'Latitude'], max, na.rm=TRUE)
Latitude
51.5895
> sapply(IP_DATA_ALL_FIX[,'Country'], max, na.rm=TRUE)
Country
 "US"
The result is 51.5895. What does this tell you?
Fact: The range in latitude for the Northern Hemisphere is from 0 to 90. So, if you do not have any latitudes
more northerly than 51.5895, you can improve your retrieve routine.
> sapply(IP_DATA_ALL_FIX [,'Latitude'], mean, na.rm=TRUE)
Latitude
46.69097
> sapply(IP_DATA_ALL_FIX [,'Latitude'], median, na.rm=TRUE)
Latitude
 48.15
> sapply(IP_DATA_ALL_FIX [,'Latitude'], range, na.rm=TRUE)
 Latitude
[1,] 40.6888
[2,] 51.5895
> sapply(IP_DATA_ALL_FIX [,'Latitude'], quantile, na.rm=TRUE)
 Latitude
0% 40.6888
25% 40.7588
50% 48.1500
75% 51.5092
100% 51.5895
> sapply(IP_DATA_ALL_FIX [,'Latitude'], sd, na.rm=TRUE)
Latitude
4.890387
> sapply(IP_DATA_ALL_FIX [,'Longitude'], sd, na.rm=TRUE)
Longitude
38.01702